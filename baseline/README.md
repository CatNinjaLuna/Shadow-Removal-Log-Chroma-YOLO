# Baseline Experiments

This directory contains scripts and results for baseline YOLO object detection experiments using standard sRGB image representations.

## Overview

The baseline establishes a performance reference for comparison with log-chromaticity and illumination-invariant methods. All training and evaluation use standard sRGB images converted from linear 16-bit TIFF files.

---

## Dataset

**Classes:** Car, Cyclist, Pedestrian, Tram, Truck

**Data Locations (Cluster):**

-  **Baseline Weights:** `https://ood.explorer.northeastern.edu/pun/sys/dashboard/files/fs//projects/SuperResolutionData/carolinali-shadowRemoval/baseline_weights`
-  **sRGB Images (16-bit TIFF):** `/projects/SuperResolutionData/carolinali-shadowRemoval/baseline/srgb`
-  **sRGB Images (8-bit JPG):** `/projects/SuperResolutionData/carolinali-shadowRemoval/baseline/jpgs`
-  **Train/Val Split:** `/projects/SuperResolutionData/carolinali-shadowRemoval/data/split_baseline/`

---

## Scripts

### 1. `convert_linear_tiff_to_srgb_jpg.py`

Converts linear 16-bit TIFF images to sRGB color space and generates both:

-  16-bit sRGB TIFF files
-  8-bit sRGB JPG files (for YOLO training)

**Usage:**

```bash
python convert_linear_tiff_to_srgb_jpg.py \
    --in_dir /projects/SuperResolutionData/driving/ROD_dataset/Dataset/LinearFiles/dataset \
    --srgb_dir /projects/SuperResolutionData/carolinali-shadowRemoval/baseline/srgb \
    --jpg_dir /projects/SuperResolutionData/carolinali-shadowRemoval/baseline/jpgs
```

### 2. `convert_labels_to_yolo_baseline.py`

Converts JSON annotation files to YOLO format (normalized bounding boxes).

**Expected Output:**

```
Classes: ['Car', 'Cyclist', 'Pedestrian', 'Tram', 'Truck']
```

**Features:**

-  Reads JSON labels with polygon/rectangle annotations
-  Converts to YOLO format: `<class_id> <x_center> <y_center> <width> <height>`
-  Normalizes coordinates to [0, 1]
-  Handles multiple image formats (.tif, .tiff, .jpg, .png)

### 3. `split_dataset_baseline.py`

Splits image-label pairs into train/val sets with stratified sampling.

**Features:**

-  Configurable train/validation ratio (default: 80/20)
-  Random seed for reproducibility
-  Preserves image-label pairing
-  Creates directory structure: `train/images`, `train/labels`, `val/images`, `val/labels`

**Usage:**

```bash
python split_dataset_baseline.py \
    --images_dir <path_to_images> \
    --labels_dir <path_to_labels> \
    --out_dir <output_directory> \
    --ratio 0.8 \
    --seed 42
```

### 4. `predict_baseline.py`

Runs inference using trained baseline YOLO weights on sRGB images.

**Configuration:**

-  **Model:** Best baseline weights from training
-  **Image Size:** 1280x1280
-  **Confidence Threshold:** 0.5
-  **IoU Threshold:** 0.6 (NMS)

**Usage:**

```bash
python predict_baseline.py
```

**Outputs:**

-  Annotated images saved to `runs/predict_baseline/exp_baseline3_predict/`
-  Optional: YOLO txt format predictions

---

## Configuration Files

### `data_baseline.yaml`

YOLO dataset configuration file specifying:

-  Training image directory
-  Validation image directory
-  Class names and IDs

**Example:**

```yaml
train: "/projects/.../split_baseline/train/images"
val: "/projects/.../split_baseline/val/images"

names:
   - Car
   - Cyclist
   - Pedestrian
   - Tram
   - Truck
```

### `args.yaml.yaml`

Training hyperparameters and model configuration (auto-generated by Ultralytics).

---

## Results

This directory includes validation metrics and visualizations from baseline training:

### Performance Metrics

-  **F1 Curve** (`F1_curve.png`)
-  **Precision-Recall Curve** (`PR_curve.png`)
-  **Precision Curve** (`P_curve.png`)
-  **Recall Curve** (`R_curve.png`)

### Confusion Matrices

-  `confusion_matrix.png` - Raw counts
-  `confusion_matrix_normalized.png` - Normalized percentages

### Validation Samples

-  `val_batch0_labels.jpg` / `val_batch0_pred.jpg` - Ground truth vs. predictions
-  `val_batch1_labels.jpg` / `val_batch1_pred.jpg`
-  `val_batch2_labels.jpg` / `val_batch2_pred.jpg`

---

## Results Analysis

### Model Performance Overview

The baseline sRGB YOLO model demonstrates strong overall performance across the five object classes (Car, Cyclist, Pedestrian, Tram, Truck). The following sections analyze key metrics and performance characteristics.

### Precision-Recall Analysis

**PR Curve (`PR_curve.png`):**
The Precision-Recall curve shows the trade-off between precision and recall across different confidence thresholds. Key observations:

-  **Cars** typically show the highest precision-recall values, as they are the most common and visually distinctive class
-  **Pedestrians and Cyclists** (smaller objects) tend to have lower precision at high recall, indicating challenges with small object detection
-  The curve shape indicates the model's ability to balance false positives and false negatives across varying confidence thresholds

**Precision Curve (`P_curve.png`):**
Shows how precision varies with confidence threshold. Important insights:

-  Higher confidence thresholds naturally yield higher precision
-  The optimal operating point balances precision with detection rate
-  Class-specific precision differences reveal which objects are most reliably detected

**Recall Curve (`R_curve.png`):**
Illustrates recall as a function of confidence threshold:

-  Low confidence thresholds maximize recall but increase false positives
-  The curve slope indicates how quickly the model loses detections as threshold increases
-  Flatter regions suggest stable detection performance across threshold ranges

### F1 Score Analysis

**F1 Curve (`F1_curve.png`):**
The F1 curve identifies optimal confidence thresholds by balancing precision and recall:

-  **Peak F1 scores** indicate the best trade-off point for each class
-  Classes with narrow peaks are sensitive to threshold selection
-  Broader peaks suggest more robust performance across thresholds

**Typical F1 Score Patterns:**

-  **Cars:** Usually achieve highest F1 scores (>0.7-0.8) due to size and frequency
-  **Trams/Trucks:** Moderate F1 scores, influenced by class imbalance
-  **Cyclists/Pedestrians:** Lower F1 scores reflect small object detection challenges

### Confusion Matrix Insights

**Raw Confusion Matrix (`confusion_matrix.png`):**
Shows absolute counts of predictions vs. ground truth:

-  **Diagonal elements** represent correct predictions (true positives)
-  **Off-diagonal elements** reveal confusion between classes
-  **Background row/column** shows false positives and false negatives

**Normalized Confusion Matrix (`confusion_matrix_normalized.png`):**
Provides percentage-based view for class balance analysis:

-  **High diagonal values** (ideally >0.8) indicate strong class-specific performance
-  **Common confusion patterns:** Similar-looking objects (e.g., Truck ↔ Car, Cyclist ↔ Pedestrian)
-  **Background confusion** reveals missed detections and false alarms

**Expected Confusion Patterns:**

-  Cars may be confused with Trucks when only partial vehicle is visible
-  Cyclists and Pedestrians may be confused in occluded or distant scenarios
-  Trams (if less frequent in training) may show lower recall

### Validation Sample Analysis

**Visual Inspection (`val_batch*.jpg` files):**

**Labels vs. Predictions Comparison:**

-  **Green boxes (labels):** Ground truth annotations
-  **Colored boxes (predictions):** Model detections with class colors

**Common Observations:**

1. **Large Objects (Cars, Trucks):**

   -  Generally well-detected with accurate localization
   -  Bounding box IoU typically >0.7

2. **Small Objects (Pedestrians, Cyclists):**

   -  More prone to missed detections (false negatives)
   -  Bounding box accuracy may be lower
   -  Performance degrades with distance and occlusion

3. **Shadow Regions:**

   -  Baseline sRGB model may show reduced detection confidence in shadows
   -  Potential for false negatives in heavily shadowed areas
   -  This motivates the log-chromaticity experiments

4. **Lighting Variations:**
   -  Detections may vary with exposure and illumination conditions
   -  Over/under-exposed regions can impact performance

### Performance Bottlenecks

**Identified Challenges:**

1. **Small object detection** - Pedestrians and cyclists at distance
2. **Occlusion handling** - Partially visible objects
3. **Shadow sensitivity** - Reduced confidence in shadowed regions (key motivation for log-chroma)
4. **Class imbalance** - Less common classes (Tram) may underperform
5. **Illumination dependence** - sRGB representation retains intensity variations

### Baseline as Reference

**Key Metrics for Comparison:**

-  **mAP@0.5:** Overall detection accuracy across classes
-  **Class-specific AP:** Individual class performance
-  **F1 scores at optimal threshold:** Best balance point
-  **Confusion patterns:** Which classes are hardest to distinguish

**Expected Improvements with Log-Chromaticity:**

-  Better shadow robustness (reduced false negatives in shadows)
-  More consistent detections across lighting conditions
-  Potential trade-offs in overall accuracy vs. illumination invariance

---

## Training Pipeline

1. **Data Preprocessing:**

   ```bash
   # Convert linear TIFF to sRGB JPG
   python convert_linear_tiff_to_srgb_jpg.py

   # Convert JSON labels to YOLO format
   python convert_labels_to_yolo_baseline.py

   # Split into train/val sets
   python split_dataset_baseline.py
   ```

2. **Training:**

   ```bash
   # Train using Ultralytics YOLO
   yolo detect train data=data_baseline.yaml model=yolov8n.pt epochs=100 imgsz=1280
   ```

3. **Evaluation:**
   ```bash
   # Run predictions on test set
   python predict_baseline.py
   ```

---

## Notes

-  All large datasets and model weights are stored on the computing cluster
-  Training was performed on NVIDIA GPUs using SLURM job scheduling
-  sRGB conversion follows standard gamma correction (γ ≈ 2.2)
-  This baseline serves as the reference for comparing log-chromaticity methods

---

## Related Directories

-  `../train_from_scratch_use_yaml/baseline_yaml/` - Alternative baseline training results
-  `../ultralytics_baseline/` - Baseline Ultralytics scripts and library
